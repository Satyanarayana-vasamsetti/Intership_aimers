{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOESQkXMW/MpR5V2ZepfgGt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satyanarayana-vasamsetti/Intership_aimers/blob/main/session1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7gtDiiI67ro",
        "outputId": "de8b6c36-7403-4b83-f1b7-3af634bea860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's what I see in the image:\n",
            "\n",
            "*   **Subject:** The main subject of the image is a bald eagle.\n",
            "*   **Features:** The eagle has a white head and neck, a brown body, and a yellow beak.\n",
            "*   **Action:** The eagle's mouth is open, and it appears to be calling out or vocalizing.\n",
            "*   **Background:** The background is blurred, suggesting a natural outdoor environment, possibly a forest.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=\"AIzaSyDwzj7XnVPDW9qCTW113fXRVIjIRaT4OMI\")\n",
        "\n",
        "my_file = client.files.upload(file=\"/eagle.jpeg\")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=[my_file, \"what is there in image\"],\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=\"AIzaSyDwzj7XnVPDW9qCTW113fXRVIjIRaT4OMI\")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=\"what is vlm\",\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZaqPDuw_hKm",
        "outputId": "b7112a64-af47-4959-9607-6bde119a501e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VLM stands for **Vision Language Model**.\n",
            "\n",
            "In simple terms, a Vision Language Model is a type of artificial intelligence model that can understand and relate both visual and textual information.  It can:\n",
            "\n",
            "*   **\"See\" and understand images:**  This means it can identify objects, scenes, and relationships within an image.\n",
            "*   **\"Read\" and understand text:** It can comprehend the meaning of sentences, paragraphs, and documents.\n",
            "*   **Connect the visual and textual:** Crucially, it can link what it sees in an image to the meaning of associated text (or vice versa).\n",
            "\n",
            "**Here's a breakdown of what VLMs can do:**\n",
            "\n",
            "*   **Image Captioning:** Given an image, generate a descriptive text caption.\n",
            "*   **Visual Question Answering (VQA):** Given an image and a question about the image, provide the correct answer.\n",
            "*   **Text-to-Image Generation:**  Given a text description, generate an image that matches the description.\n",
            "*   **Image Classification:** Categorize images based on their content. VLMs can use textual descriptions to fine-tune and improve the accuracy of image classification tasks.\n",
            "*   **Object Detection with Text:** Find specific objects within an image based on textual descriptions.\n",
            "*   **Visual Reasoning:** Perform complex reasoning tasks involving both images and text.\n",
            "*   **Multi-modal Search:** Search for images based on textual queries (and vice versa).\n",
            "\n",
            "**Examples of popular VLMs:**\n",
            "\n",
            "*   **CLIP (Contrastive Language-Image Pre-training):**  Developed by OpenAI, CLIP is a foundational VLM that learns to associate images and text.\n",
            "*   **DALL-E and DALL-E 2:**  Also from OpenAI, these models are known for their ability to generate highly realistic and creative images from text descriptions.\n",
            "*   **Imagen:** Google's text-to-image diffusion model, known for its photorealistic image generation.\n",
            "*   **Flamingo:** A VLM that can handle a variety of tasks, including few-shot learning (learning from very few examples).\n",
            "*   **PaLI (Pathways Language and Image):** Another Google model that excels in various vision-language tasks.\n",
            "*   **BLIP (Bootstrapping Language-Image Pre-training):** Aims to improve the efficiency and performance of VLMs.\n",
            "*   **LLaVA (Large Language and Vision Assistant):** combines a pretrained large language model with a vision encoder to create a powerful and flexible VLM.\n",
            "\n",
            "**Why are VLMs important?**\n",
            "\n",
            "VLMs are essential for developing AI systems that can interact with the world in a more natural and human-like way. They bridge the gap between vision and language, enabling computers to understand and reason about the world as we do. They have applications in a wide range of fields, including:\n",
            "\n",
            "*   **Robotics:**  Helping robots understand their environment and follow instructions.\n",
            "*   **Search Engines:** Improving image and video search results.\n",
            "*   **Accessibility:** Assisting visually impaired individuals by describing images.\n",
            "*   **Content Creation:**  Generating images and videos for marketing, education, and entertainment.\n",
            "*   **Healthcare:** Assisting in medical image analysis.\n",
            "*   **Autonomous Driving:** Helping self-driving cars understand their surroundings.\n",
            "\n",
            "In summary, a Vision Language Model is a powerful AI tool that combines the capabilities of computer vision and natural language processing to understand and relate visual and textual information. They are becoming increasingly important as AI systems become more sophisticated and are used in a wider range of applications.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import pathlib\n",
        "\n",
        "# Replace with your actual API key\n",
        "genai.configure(api_key=\"AIzaSyDwzj7XnVPDW9qCTW113fXRVIjIRaT4OMI\")\n",
        "\n",
        "# You can still keep this for generating content later\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\") # Or \"gemini-1.5-pro\" for larger context\n",
        "\n",
        "# --- Step 1: Upload the PDF using the File API ---\n",
        "pdf_path = pathlib.Path(\"/Ethics_Decision_Tree.pdf\") # Replace with your PDF path\n",
        "\n",
        "# For local files:\n",
        "# Access the file upload functionality directly through the genai module\n",
        "uploaded_file = genai.upload_file(path=pdf_path) # Use path argument for pathlib objects\n",
        "\n",
        "print(f\"Uploaded file '{uploaded_file.name}' (URI: {uploaded_file.uri})\")\n",
        "\n",
        "# --- Step 2: Use the uploaded file in your prompt ---\n",
        "prompt = input()\n",
        "\n",
        "# Now use the 'model' object to generate content\n",
        "response = model.generate_content(\n",
        "    contents=[\n",
        "        uploaded_file, # Reference the uploaded file object directly\n",
        "        prompt\n",
        "    ]\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "fe7ey9tMBwgd",
        "outputId": "010e2d0b-9875-4839-e8d6-d8536a8a84df"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded file 'files/wjj5vo5wq5un' (URI: https://generativelanguage.googleapis.com/v1beta/files/wjj5vo5wq5un)\n",
            "what is main theme of pdf\n",
            "The main theme of the provided PDF is a decision-making flowchart for navigating ethical dilemmas.  It presents a series of questions to ask oneself to determine whether a course of action is ethically sound, encouraging consultation with superiors if uncertainty remains.  The ultimate goal is to ensure decisions align with company code and legal principles, preventing actions with serious legal ramifications.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}